\documentclass[14pt, a4paper]{extarticle}

\usepackage[utf8]{inputenc}
\usepackage[T2A]{fontenc}
\usepackage[russian]{babel}
\usepackage{indentfirst}
\usepackage{geometry}
\usepackage{amsmath, amsfonts, amssymb}
\usepackage{graphicx}
\usepackage{float}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{cmap}
\usepackage{titlesec}

\usepackage{mathptmx} 

\geometry{left=3cm, right=1.5cm, top=2cm, bottom=2cm}

\numberwithin{equation}{section}

\usepackage{tocloft}
\renewcommand{\cftsecleader}{\cftdotfill{\cftdotsep}}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
}
\lstset{
    style=mystyle,
    extendedchars=true,
    literate={а}{{\selectfont\char224}}1
             {б}{{\selectfont\char225}}1
             {в}{{\selectfont\char226}}1
             {г}{{\selectfont\char227}}1
             {д}{{\selectfont\char228}}1
             {е}{{\selectfont\char229}}1
             {ё}{{\"e}}1
             {ж}{{\selectfont\char230}}1
             {з}{{\selectfont\char231}}1
             {и}{{\selectfont\char232}}1
             {й}{{\selectfont\char233}}1
             {к}{{\selectfont\char234}}1
             {л}{{\selectfont\char235}}1
             {м}{{\selectfont\char236}}1
             {н}{{\selectfont\char237}}1
             {о}{{\selectfont\char238}}1
             {п}{{\selectfont\char239}}1
             {р}{{\selectfont\char240}}1
             {с}{{\selectfont\char241}}1
             {т}{{\selectfont\char242}}1
             {у}{{\selectfont\char243}}1
             {ф}{{\selectfont\char244}}1
             {х}{{\selectfont\char245}}1
             {ц}{{\selectfont\char246}}1
             {ч}{{\selectfont\char247}}1
             {ш}{{\selectfont\char248}}1
             {щ}{{\selectfont\char249}}1
             {ъ}{{\selectfont\char250}}1
             {ы}{{\selectfont\char251}}1
             {ь}{{\selectfont\char252}}1
             {э}{{\selectfont\char253}}1
             {ю}{{\selectfont\char254}}1
             {я}{{\selectfont\char255}}1
             {А}{{\selectfont\char192}}1
             {Б}{{\selectfont\char193}}1
             {В}{{\selectfont\char194}}1
             {Г}{{\selectfont\char195}}1
             {Д}{{\selectfont\char196}}1
             {Е}{{\selectfont\char197}}1
             {Ё}{{\"E}}1
             {Ж}{{\selectfont\char198}}1
             {З}{{\selectfont\char199}}1
             {И}{{\selectfont\char200}}1
             {Й}{{\selectfont\char201}}1
             {К}{{\selectfont\char202}}1
             {Л}{{\selectfont\char203}}1
             {М}{{\selectfont\char204}}1
             {Н}{{\selectfont\char205}}1
             {О}{{\selectfont\char206}}1
             {П}{{\selectfont\char207}}1
             {Р}{{\selectfont\char208}}1
             {С}{{\selectfont\char209}}1
             {Т}{{\selectfont\char210}}1
             {У}{{\selectfont\char211}}1
             {Ф}{{\selectfont\char212}}1
             {Х}{{\selectfont\char213}}1
             {Ц}{{\selectfont\char214}}1
             {Ч}{{\selectfont\char215}}1
             {Ш}{{\selectfont\char216}}1
             {Щ}{{\selectfont\char217}}1
             {Ъ}{{\selectfont\char218}}1
             {Ы}{{\selectfont\char219}}1
             {Ь}{{\selectfont\char220}}1
             {Э}{{\selectfont\char221}}1
             {Ю}{{\selectfont\char222}}1
             {Я}{{\selectfont\char223}}1
}

\linespread{1.3} 

\begin{document}

\begin{titlepage}
\begin{center}
МИНИСТЕРСТВО НАУКИ И ВЫСШЕГО ОБРАЗОВАНИЯ РФ \\[0.5cm]
Федеральное государственное бюджетное образовательное учреждение высшего образования
\vskip 0.2in
«Московский Авиационный Институт» \\
(Национальный Исследовательский Университет)

\vskip 1.0in
Институт: №8 «Компьютерные науки и прикладная математика» \\
Кафедра: 806 «Вычислительная математика и программирование»

\vfill
\vfill

{\Huge \bf
Лабораторные работы \\
}
\vskip 0.2in
{\large \bf по курсу «Информационный поиск»}
\vskip 0.2in

\vfill
\vfill

\begin{flushright}
\begin{tabular}{l l}
Группа: & М8О-409Б-22 \\
Студент: & Юсуфов Р. Г. \\
Преподаватель: & Кухтичев А.А. \\
Дата: &  \\
Оценка: & \\
\end{tabular}
\end{flushright}

\vfill

Москва, 2025
\end{center}
\end{titlepage}


\newpage
\section*{Введение}
\addcontentsline{toc}{section}{Введение}

Целью данного цикла лабораторных работ является разработка полнофункциональной поисковой системы, способной индексировать и осуществлять поиск по большому корпусу документов (более 50 000 статей).

В ходе выполнения работ планируется реализация следующих этапов:
\begin{enumerate}
    \item \textbf{Сбор и подготовка данных:} Создание поискового робота (crawler) для автоматического сбора корпуса документов единой тематики, сохранение сырых данных и метаинформации.
    \item \textbf{Обработка текста (NLP):} Реализация модулей токенизации, очистки от HTML-разметки, стемминга (приведения слов к основе) и фильтрации стоп-слов.
    \item \textbf{Индексация:} Построение инвертированного индекса (Inverted Index) — основной структуры данных для быстрого поиска.
    \item \textbf{Сжатие индекса:} Применение алгоритмов сжатия (VarByte / Simple9) для оптимизации потребления оперативной памяти.
    \item \textbf{Реализация поиска:} Разработка алгоритмов Булева поиска (AND, OR, NOT) и ранжированного поиска с использованием метрики TF-IDF.
    \item \textbf{Тестирование и оценка:} Покрытие кода модульными тестами, оценка качества поиска с использованием метрик и закона Ципфа.
\end{enumerate}

Результатом работы станет программный комплекс, предоставляющий веб-интерфейс и CLI-утилиту для поиска по выбранной предметной области.

\newpage

\section{Добыча корпуса документов}

\subsection{Цель работы}
Подготовить корпус текстовых документов, достаточный для выполнения последующих лабораторных работ (анализ, индексация, поиск). Провести статистический анализ собранных данных.

В качестве источника документов был выбран архив научных статей \textbf{arXiv.org} (зеркало \textbf{ar5iv.org}).
\\
\textbf{Тематика:} Computer Science (cs). \\
\textbf{Обоснование выбора:}
\begin{itemize}
    \item Категория содержит сотни тысяч статей, что позволяет гибко варьировать размер корпуса (от 30 000 до 1 000 000 документов).
    \item Использование зеркала ar5iv.org позволяет получать статьи в формате HTML5, сохраняя семантическую структуру (заголовки, разделы, формулы), в отличие от трудно разбираемого формата PDF.
    \item Научные статьи содержат богатую лексику, сложную терминологию и минимум спама, что идеально подходит для задач информационного поиска.
\end{itemize}

\subsection{Процесс сбора данных}
Для сбора данных был разработан скрипт на языке Python. Процесс разделен на два этапа:
\begin{enumerate}
    \item Получение списка идентификаторов статей (ID) через OAI-PMH протокол (API arxiv.org). Были отобраны статьи, опубликованные начиная с 2023 года, для обеспечения актуальности и лучшей конвертации в HTML.
    \item Многопоточная загрузка HTML-версий статей с ресурса ar5iv.org.
\end{enumerate}

Пример скрипта загрузки (фрагмент):
\begin{lstlisting}[language=Python]
def download_worker(args):
    url = f"{AR5IV_BASE}{arxiv_id}"
    try:
        resp = requests.get(url, headers=headers)
        if resp.status_code == 200:
            with open(filename, "w") as f:
                f.write(resp.text)
\end{lstlisting}

\subsection{Анализ и очистка данных}
Полученные документы представляют собой полные HTML-страницы. Для задач поиска необходимо отделить полезный контент от служебной разметки.
\\
\textbf{Алгоритм очистки:}
\begin{itemize}
    \item Удаление тегов \texttt{<script>}, \texttt{<style>}, \texttt{<nav>}, \texttt{<footer>}.
    \item Замена математических формул (MathML) на специальный токен \texttt{[FORMULA]}.
    \item Извлечение текста из блочных тегов.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{1.1.png}

    \caption{Фрагмент очищенного HTML документа}
    \label{fig:raw_html}
\end{figure}

\subsection{Анализ существующих поисковых систем}
Для корпуса arXiv существуют следующие решения:
\begin{enumerate}
    \item \textbf{Встроенный поиск Arxiv.org:} Ограничен поиском по метаданным (Title, Abstract, Authors). Не позволяет искать по полному тексту, формулам или коду внутри статьи.
    \item \textbf{Google Scholar:} Хорошо ищет по цитируемости, но не дает гибких инструментов для структурного поиска (например, "найти термин Х строго во Введении").
\end{enumerate}
Разрабатываемая система призвана устранить эти недостатки за счет полного индексирования содержимого HTML-версий статей.

\subsection{Статистика корпуса}
На момент написания отчета (промежуточный этап загрузки) получены следующие данные:

\begin{table}[H]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Параметр} & \textbf{Значение} \\ \hline
Количество скачанных документов & 74 000 \\ \hline
Общий объем ("сырой" HTML с Base64) & $\approx$ 77 ГБ \\ \hline
Средний размер файла (HTML) & $\approx$ 1.2 МБ \\ \hline
Прогнозируемый объем чистого текста & $\approx$ 3--4 ГБ \\ \hline
Среднее кол-во слов в документе & $\approx$ 8000 слов \\ \hline
\end{tabular}
\caption{Статистические характеристики корпуса}
\label{tab:stats}
\end{table}

\section {Поисковый робот}

\subsection{Цель работы}
Разработать конфигурируемый поисковый робот (crawler) для автоматического сбора, сохранения и обновления документов в базе данных. Реализовать механизм остановки и возобновления работы без потери данных.

\subsection{Архитектура робота}
Разработанное решение имеет модульную архитектуру, разделяющую ответственность между компонентами:
\begin{enumerate}
    \item \textbf{Модуль сбора (Fetcher):} Отвечает за взаимодействие с API источника (OAI-PMH) и многопоточную загрузку «сырых» данных на локальный диск.
    \item \textbf{Модуль хранения (Storage Worker):} Отвечает за перенос скачанных данных в NoSQL базу данных MongoDB с контролем целостности и дубликатов.
    \item \textbf{Конфигурация (Config):} Единый YAML-файл, управляющий параметрами подключения к БД и путями к данным.
\end{enumerate}

\subsection{Конфигурация}
Управление параметрами робота вынесено в файл \texttt{config.yaml}, что позволяет менять настройки без перекомпиляции кода.

\begin{lstlisting}[language=yaml, caption=Файл конфигурации config.yaml]
db:
  host: "localhost"
  port: 27017
  name: "search_engine"
  collection: "documents"

data:
  source_dir: "./corpus_arxiv_html"
  source_name: "ar5iv.org"
  
\end{lstlisting}

\subsection{Реализация логики Stop/Resume (Остановка и продолжение)}
Ключевым требованием к системе является возможность прерывания процесса в любой момент и корректного продолжения работы при перезапуске. Данный механизм реализован на обоих этапах работы робота:

\textbf{1. Этап скачивания (Сетевой уровень):} \\
Перед загрузкой каждого файла робот проверяет его наличие и размер на локальном диске. Если файл уже успешно скачан, он пропускается. Это позволяет перезапускать скачивание при обрыве связи, не нагружая сервер повторными запросами.

\textbf{2. Этап сохранения (Уровень БД):} \\
В базе данных MongoDB создан уникальный индекс (Unique Index) по полю \texttt{url}. При попытке вставки документа робот использует стратегию следующую:
\begin{itemize}
    \item Скрипт пытается записать пакет документов.
    \item Если документ с таким URL уже существует, база данных сообщает о дубликате.
    \item Робот игнорирует дубликаты и продолжает запись только новых документов.
\end{itemize}

Таким образом, систему можно останавливать и запускать многократно — в базе данных гарантированно не появится дублей, а работа продолжится ровно с того документа, на котором произошла остановка.

\subsection{Схема данных в MongoDB}
Документы сохраняются в формате BSON со следующими полями:
\begin{itemize}
    \item \texttt{url} (String, Unique) — нормализованная ссылка на источник.
    \item \texttt{html} (String) — полное «сырое» содержимое HTML-страницы (включая скрипты, стили и base64-изображения, согласно требованиям).
    \item \texttt{source} (String) — источник (ar5iv.org).
    \item \texttt{crawled\_at} (Int64) — временная метка скачивания (Unix Timestamp).
\end{itemize}

\subsection{Результаты работы}
Робот успешно обработал собранный корпус документов. Ниже представлен скриншот из графического интерфейса MongoDB Compass, подтверждающий наличие данных в базе.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{2.1.png}
    \caption{Интерфейс MongoDB Compass с загруженными документами}
    \label{fig:mongo}
\end{figure}

\section{Токенизация и Закон Ципфа}

\subsection{Цель работы}
Разработать модуль токенизации (разбиения текста на слова) с учетом особенностей HTML-разметки. Провести частотный анализ корпуса и проверить выполнение закона Ципфа.

\subsection{Алгоритм токенизации} 
Использован потоковый подход: программа рекурсивно обходит директории с документами, считывает содержимое в буфер и производит однопроходный парсинг.

\textbf{Правила обработки текста:}
\begin{enumerate}
    \item \textbf{Очистка HTML:} Реализован конечный автомат (State Machine), отслеживающий нахождение внутри тегов (\texttt{<...>}). Содержимое тегов игнорируется, обрабатывается только видимый текст.
    \item \textbf{Case Folding:} Все символы приводятся к нижнему регистру для унификации (например, "The" и "the" считаются одним токеном).
    \item \textbf{Фильтрация символов:} В качестве токенов принимаются последовательности букв и цифр (\texttt{std::isalnum}). Любые другие символы (пробелы, знаки препинания, спецсимволы) считаются разделителями.
    \item \textbf{Фильтрация по длине:} Отбрасываются токены короче 2 символов (шум, предлоги) и длиннее 25 символов (вероятный мусор, Base64-фрагменты).
\end{enumerate}

\subsection{Результаты токенизации}
Обработка корпуса заняла \textbf{5700 секунд}.
\\
\textbf{Статистика:}
\begin{itemize}
    \item Обработано документов: \textbf{74000}    \item Общий объем обработанных данных: \textbf{3.5 ГБ}
    \item Скорость обработки: \textbf{138 МБ/сек}
\end{itemize}

\subsection{Закон Ципфа}
Для проверки закона Ципфа был построен график распределения частот слов в двойном логарифмическом масштабе (log-log scale).
Согласно закону, частота $n$-го по популярности слова должна быть обратно пропорциональна его рангу ($f \sim \frac{1}{r}$). В логарифмическом масштабе это соответствует прямой линии с наклоном -1.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{3.1.png}
    \caption{Проверка закона Ципфа для корпуса Arxiv}
    \label{fig:zipf}
\end{figure}

\textbf{Анализ графика:}
Полученное распределение (синие точки) визуально совпадает с идеальной кривой Ципфа (красный пунктир).
\begin{itemize}
    \item В области высоких рангов (самые частые слова: \textit{the, of, and}) наблюдается небольшое отклонение, что характерно для естественных языков (стоп-слова).
    \item В средней части графика зависимость строго линейная.
    \item В "хвосте" распределения (низкочастотные слова) наблюдается дискретизация, что также соответствует теории.
\end{itemize}
Вывод: собранный корпус является качественным образцом естественного языка.

\newpage
\section{Стемминг}

\subsection{Цель работы}
Реализовать алгоритм стемминга для уменьшения размера словаря и повышения полноты поиска. Создать "нормализованный" корпус документов.

\subsection{Выбор алгоритма}
Для выполнения задачи был выбран алгоритм Мартина Портера (Porter Stemmer). Это стандарт де-факто для английского языка, который последовательно удаляет суффиксы слов на основе набора правил (например, \texttt{-ational} $\rightarrow$ \texttt{-ate}, \texttt{-ing} $\rightarrow$ $\emptyset$).
Алгоритм реализован на C++ как отдельный модуль, интегрированный в конвейер токенизации.

\subsection{Модификация процесса обработки}
В дополнение к правилам токенизации из ЛР3, каждый полученный токен теперь проходит через стеммер перед попаданием в индекс. 
Кроме того, реализовано сохранение результатов обработки:
\begin{itemize}
    \item Исходные HTML-файлы конвертируются в текстовые файлы (\texttt{.txt}) с сохранением структуры директорий.
    \item В выходные файлы записываются уже нормализованные (стеммированные) термины.
    \item Этот "очищенный" корпус (\texttt{corpus\_clean}) будет использован в последующих работах для ускорения построения индекса.
\end{itemize}

\subsection{Сравнительный анализ}
Применение стемминга позволило сократить размер словаря, объединив различные грамматические формы одного слова в один терм.

\begin{table}[H]
\centering
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{Параметр} & \textbf{Без стемминга} & \textbf{Со стеммингом} & \textbf{Изменение} \\ \hline
Total Tokens & 281 728 132 & 281 748 036 & $\approx$ 0\% \\ \hline
Vocabulary Size & 859 070 & 784 091 & \textbf{-8.7\%} \\ \hline
Время обработки & 3461 сек & 5700 сек & +34\% \\ \hline
\end{tabular}
\caption{Эффективность стемминга}
\label{tab:stemming}
\end{table}

\textbf{Примеры объединения:}
\begin{itemize}
    \item \texttt{computation, computers, computing} $\rightarrow$ \texttt{comput}
    \item \texttt{generated, generation, generat} $\rightarrow$ \texttt{generat}
\end{itemize}


\section{Построение булева индекса}

\subsection{Цель работы}
Разработать структуру данных "Инвертированный индекс" (Inverted Index) для быстрого поиска документов по ключевым словам. Реализовать необходимые структуры данных.

\subsection{Разработка собственных структур данных}
В соответствии с требованиями, на языке C++ был реализован \item \texttt{MyHashMap<K, V>}: Хеш-таблица для хранения пар "Слово - Список ID". Использует метод цепочек для разрешения коллизий. В качестве хеш-функции для строк использован алгоритм \textit{djb2}.

\subsection{Алгоритм индексации}
Процесс построения индекса состоит из следующих этапов:
\begin{enumerate}
    \item \textbf{Чтение:} Программа обходит директорию с очищенными текстовыми файлами (\texttt{corpus\_clean}).
    \item \textbf{Маппинг документов:} Каждому файлу присваивается уникальный числовой идентификатор (\texttt{doc\_id}). Соответствие "ID $\leftrightarrow$ Путь к файлу" сохраняется в \texttt{docs\_map.txt}.
    \item \textbf{Инвертирование:} Для каждого слова в документе обновляется запись в хеш-таблице. В список постинга (posting list) добавляется текущий \texttt{doc\_id}.
    \item \textbf{Сохранение:} Итоговый индекс выгружается в файл \texttt{index.bin} в бинарном формате
\end{enumerate}

\subsection{Результаты}
Построен индекс для всего корпуса документов.
\begin{itemize}
    \item Размер файла индекса: 550 МБ
    \item Время построения:  980 сек.
\end{itemize}
Индекс готов к использованию в поисковой системе.

\section{Булев поиск}

\subsection{Цель работы}
Реализовать алгоритм обработки поисковых запросов с поддержкой булевых операторов (AND, OR, NOT) и группировки (скобки).

\subsection{Парсинг запросов}
Для обработки сложных запросов вида \texttt{(AI || ML) \&\& !Python} был реализован парсер на основе алгоритма "Сортировочная станция" (Shunting-yard algorithm).
Процесс обработки запроса:
\begin{enumerate}
    \item \textbf{Токенизация:} Строка разбивается на операторы (\texttt{\&\&}, \texttt{||}, \texttt{!}, \texttt{(}, \texttt{)}) и термины.
    \item \textbf{Преобразование в RPN:} Инфиксная запись преобразуется в Обратную Польскую Нотацию (RPN). Пример: \texttt{A || B} $\rightarrow$ \texttt{A B ||}.
    \item \textbf{Стемминг терминов:} Все слова запроса обрабатываются стеммером Портера (как и при индексации).
\end{enumerate}

\subsection{Выполнение запроса}
Поиск выполняется путем вычисления RPN-выражения на стеке.
Операции над множествами идентификаторов документов ($DocID$):
\begin{itemize}
    \item \textbf{AND (Пересечение):} Алгоритм слияния двух отсортированных списков ($O(N+M)$).
    \item \textbf{OR (Объединение):} Слияние с удалением дубликатов.
    \item \textbf{NOT (Разность):} Вычитание множества из полного списка документов.
\end{itemize}

\subsection{Результаты}
Разработанная система корректно находит документы, соответствующие сложной логике запроса. Время выполнения для типичных запросов составляет менее 10 мс (благодаря бинарному индексу в памяти).

\section{Сжатие индекса}

\subsection{Цель работы}
Уменьшить объем потребляемой памяти за счет сжатия списков постингов (Posting Lists).

\subsection{Метод сжатия}
Использована комбинация методов \textbf{Delta Encoding} и \textbf{VarByte}.
\begin{enumerate}
    \item \textbf{Delta Encoding:} Списки $DocID$ хранятся как последовательность разностей между соседними элементами ($d_i = id_i - id_{i-1}$). Это превращает большие идентификаторы в малые числа.
    \item \textbf{VarByte:} Каждое число кодируется переменным количеством байт (от 1 до 5). Старший бит байта используется как флаг продолжения.
\end{enumerate}

\subsection{Результаты сжатия}
\begin{table}[H]
\centering
\begin{tabular}{|l|l|l|}
\hline
\textbf{Параметр} & \textbf{Raw} & \textbf{Compressed} \\ \hline
Размер индекса & $\approx$ 550 МБ & $\approx$ 190 МБ \\ \hline
Коэффициент сжатия & 1.0 & \textbf{2.8x} \\ \hline
\end{tabular}
\caption{Эффективность сжатия}
\end{table}

Сжатие позволило сократить размер индекса почти в 3 раза, что критически важно для размещения индекса в оперативной памяти.

\section{Ранжирование TF-IDF}

\subsection{Цель работы}
Реализовать сортировку результатов поиска по релевантности с использованием метрики TF-IDF.

\subsection{Модификация индекса}
Для поддержки ранжирования структура индекса была расширена. Теперь для каждого термина хранится не только список документов, но и частота термина в каждом документе ($TF$).
Формат записи в \texttt{index.bin}:
\texttt{[Term] [Compressed DocIDs] [Compressed TFs]}

\subsection{Алгоритм ранжирования}
При поиске для каждого найденного документа вычисляется вес $Score$:
$$ Score(D, Q) = \sum_{t \in Q} TF(t, D) \times IDF(t) $$
где $IDF(t) = \log(\frac{N}{df_t})$.
Результаты сортируются по убыванию $Score$.

\subsection{Пример работы}
Запрос: \texttt{neural networks}.
Система находит 5000 документов. В топе выдачи оказываются статьи, где эти термины встречаются часто (высокий TF) и в заголовках, а не случайные упоминания.

\newpage
\section{Автотесты}

\subsection{Цель работы}
Обеспечить надежность ключевых компонентов системы с помощью модульного тестирования (Unit Testing).

\subsection{Реализация}
Разработан набор тестов (\texttt{tests.cpp}), покрывающий следующие модули:
\begin{enumerate}
    \item \textbf{VarByte:} Проверка кодирования/декодирования граничных значений (127, 128, $2^{32}-1$).
    \item \textbf{Stemmer:} Проверка корректности усечения суффиксов на тестовых словах.
    \item \textbf{Tokenizer:} Интеграционный тест на обработку HTML и фильтрацию мусора.
    \item \textbf{TF-IDF:} Проверка математической корректности формулы весов.
\end{enumerate}


\newpage
\section*{Заключение}
В ходе выполнения курсового проекта была спроектирована и разработана полнофункциональная информационно-поисковая система, ориентированная на работу с корпусом научных статей (Arxiv Computer Science). Объем индексируемой коллекции составил 74 000 документов.

В рамках работы были решены следующие ключевые задачи:
\begin{enumerate}
    \item \textbf{Сбор данных (Crawling):} Реализован отказоустойчивый механизм сбора данных через API OAI-PMH и зеркала ar5iv.org, обеспечивший скачивание "сырых"  HTML-версий статей с сохранением структуры.
    \item \textbf{Лингвистическая обработка:} Разработан конвейер препроцессинга, включающий эффективную очистку HTML, токенизацию и стемминг (алгоритм Портера). Это позволило сократить размер словаря на ~10\% и повысить полноту поиска за счет объединения словоформ. Проверка закона Ципфа подтвердила естественную природу распределения терминов в корпусе.
    \item \textbf{Индексация и хранение:} Создан собственный формат бинарного инвертированного индекса. Применение алгоритмов сжатия \textbf{VarByte} и \textbf{Delta Encoding} позволило сократить объем занимаемого индексом дискового пространства в 3 раза по сравнению с несжатым представлением, обеспечив возможность полной загрузки индекса в оперативную память.
    \item \textbf{Поиск и ранжирование:} Реализован гибридный механизм поиска, объединяющий строгую булеву логику (поддержка операторов AND, OR, NOT, скобок через RPN-парсер) и вероятностное ранжирование \textbf{TF-IDF}. Это позволяет пользователю находить документы, максимально релевантные запросу, а не просто содержащие ключевые слова.
    \item \textbf{Надежность:} Ключевые алгоритмические модули (сжатие, стемминг, математика ранжирования) покрыты модульными тестами, что гарантирует корректность работы системы.
\end{enumerate}

В процессе работы было получено практическое понимание принципов построения поисковых движков: от низкоуровневой работы с байтами при сжатии до высокоуровневой логики ранжирования и обработки естественно-языковых запросов. Разработанная система демонстрирует высокую производительность и масштабируемость в рамках учебных задач.


\end{document}